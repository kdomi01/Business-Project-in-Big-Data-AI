# -*- coding: utf-8 -*-
"""truthfulness_score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTeKkz_h-OeNZ4uflth5XkYohDR_pGyx
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
import numpy as np

train_df = pd.read_csv("/content/train.csv")
test_df = pd.read_csv("/content/test.csv")

label_mapping = {
    0: 0.0,
    1: 0.2,
    2: 0.4,
    3: 0.6,
    4: 0.8,
    5: 1.0
}

train_df['combined_text'] = train_df['statement'].fillna('') + ' ' + train_df['justification'].fillna('')
test_df['combined_text'] = test_df['statement'].fillna('') + ' ' + test_df['justification'].fillna('')

train_df['label'] = pd.to_numeric(train_df['label'], errors='coerce')
test_df['label'] = pd.to_numeric(test_df['label'], errors='coerce')

train_df = train_df.dropna(subset=['label'])
test_df = test_df.dropna(subset=['label'])

train_df['numerical_label'] = train_df['label'].map(label_mapping)
test_df['numerical_label'] = test_df['label'].map(label_mapping)

train_df = train_df.dropna(subset=['numerical_label'])
test_df = test_df.dropna(subset=['numerical_label'])

train_df = train_df[train_df['combined_text'].str.strip() != '']
test_df = test_df[test_df['combined_text'].str.strip() != '']

X_train = train_df['combined_text']
y_train = train_df['numerical_label']
X_test = test_df['combined_text']
y_test = test_df['numerical_label']

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english')),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

def interpret_score(score):
    if 0.00 <= score <= 0.09:
        return "Pants-on-Fire"
    elif 0.10 <= score <= 0.29:
        return "False"
    elif 0.30 <= score <= 0.49:
        return "Barely-true"
    elif 0.50 <= score <= 0.69:
        return "Half-true"
    elif 0.70 <= score <= 0.89:
        return "Mostly-true"
    elif 0.90 <= score <= 1.00:
        return "True"
    else:
        return "N/A"

print(f"\nModel Evaluation:")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (RÂ²): {r2:.4f}")

print("\n--- Sample Predictions with Nuanced Interpretation ---")
for i in range(5):
    statement = test_df['statement'].iloc[i]
    actual_label = test_df['numerical_label'].iloc[i]
    predicted_score = y_pred[i]
    predicted_label = interpret_score(predicted_score)

    print(f"Statement: {statement[:70]}...")
    print(f"Actual Score: {actual_label:.2f}")
    print(f"Predicted Score: {predicted_score:.2f}")
    print(f"Interpreted Label: {predicted_label}\n")